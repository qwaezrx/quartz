
In early days of deep learning, people used to worry a lot about the [[Optimization algorithms for training NN]] getting stuck in bad local optima. But as theory of [[DL]] has advanced, understanding of [[Local optima]] is also changing.

In very high dimension, local optima is not usually a problem.
Actually, local optima is hard to be created because all the dimension should be in a similar shape to be a local optima.

Instead of local optima, [[Plateaus]] can be a problem and make training really slow.

## Summary
- Unlikely to get stuck in a bad local optima
- Plateaus can make learning slow


