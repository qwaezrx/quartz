
One of the problems in training [[Neural Network]]s, especially in training [[DNN]], is vanishing and exploding gradients.

This means that when training very deep neural networks, your derivatives or your slopes can sometimes get either very, very high or very, very low.

## Vanishing gradients

![[Pasted image 20230901010013.png]]


