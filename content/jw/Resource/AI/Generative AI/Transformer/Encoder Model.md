---
aliases:
  - Masked Language Model
  - Masked Language Modeling
---


Predicting the probability of a masked token given context information


## Masked Language Models
- [[BERT]]
	- [[RoBERTa]]
		- Same architecture as BERT
		- Improves performance by increasing the amount of pre-training data and incorporating more challenging pre-training objectives.
	- [[XL-Net]]
		- incorporates permutation operations to change the prediction order for each training iteration, allowing the model to learn more information across tokens.


```dataview
TABLE date, parameters, author
FROM #Transformer/Encoder AND #AI_Model 
```

