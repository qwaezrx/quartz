---
tags:
  - AI/Transformer
date: 2019-10-29
author: Meta
---

Blends the bidirectional encoder from [[BERT]] and autoregressive decoder from [[GPT]], allowing it to leverage the bidirectional modeling abilities of the encoder while retraining the autoregressive properties for generation tasks.

## See More
- [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461)
