---
tags:
  - AI/MoE
  - AI/Transformer/EncoderDecoder/T5
date: 2021-01-11
author: Google
---
- Uses sparse [[T5]] encoder-decoder architecture, where the [[MLP]] are replaced by a [[MoE]]

## See More
- [Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity(Arxiv)](https://arxiv.org/abs/2101.03961)
- https://huggingface.co/docs/transformers/model_doc/switch_transformers