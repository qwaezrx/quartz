---
tags:
  - AI/Transformer/Decoder/GPT
author: OpenAI
---

>[[Transformer]]-based model that utilizes [[Self-attention]] mechanisms to process all words in a sequence simultaneously.

## Generative Pre-trained Transformer

- Decoder only [[Transformer]] model ([[Decoder Model]])
- Uni-directional
- Trained on next word prediction task based on previous words, allowing it to generate coherent text.
- Left-to-right training method
- Scaled up model parameters and leveraging diverse datasets beyond basic web text.


## GPT Models
- [[GPT-2]]
- [[GPT-3]]
- [[InstructGPT]]
- [[ChatGPT]]
