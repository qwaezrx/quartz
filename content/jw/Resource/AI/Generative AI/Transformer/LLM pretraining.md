---
tags:
  - AI/GAI/LLM
  - AI/Training
---

- Model at the start:
	- Zero knowledge about the world
	- Can't form English words
- Next token prediction
- Giant corpus of text data
- Often scraped from the internet:
	"unlabeled"
- [[Self-supervised learning]]
- After training
	- Learns language
	- Learns knowledge


- Often not publicized how to pretrain
- Open-source pretraining data: "The Pile"
- Expensive & time-consuming to train

- [[Fine-tuning LLM]] usually refers to training further
	- Can also be self-supervised unlabeled data
	- Can be "labeled" data you curated
	- Much less data needed

[[Pre-trained model]]
