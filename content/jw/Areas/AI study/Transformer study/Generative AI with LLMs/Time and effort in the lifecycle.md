---
tags:
  - AI/GAI/LLM
---


|                   | __Pre-training__                                                                                                                                                | Prompt engineering                                | Prompt tuning and fine-tuning                                                                         | RL/human feedback                                                                                                           | Compression/ optimization/ deployment                                                                             |
| ----------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------- | ----------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------- |
| Training duration | Days to weeks to months                                                                                                                                         | Not required                                      | Minutes to hours                                                                                      | Minutes to hours similar to fine-tuning                                                                                     | Minutes to hours                                                                                                  |
| Customization     | Determines model architecture, size, and tokenizer<br><br>Choose vocabulary size and # of tokens for input/context <br><br>Large amount of domain training data | No model weights<br><br>Only prompt customization | Tune for specific tasks<br><br>Add domain-specific data <br><br> Update LLM model or adaptive weights | Need separate reward model to align with human goals (helpful, honest, harmless)<br><br>Update LLM model or adapter weights | Reduce model size through model pruning, weight quantization, distillation<br><br>Smaller size, faster inference. |
| Objective         | Next-token prediction                                                                                                                                           | Increase task performance                         | Increase task performance                                                                             | Increase alignment with human preferences                                                                                   | Increase inference performance                                                                                    |
| Expertise         | High                                                                                                                                                            | Low                                               | Medium                                                                                                | Medium-high                                                                                                                 | Medium                                                                                                            |

