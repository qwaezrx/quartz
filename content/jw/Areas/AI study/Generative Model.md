---
tags:
  - AI/GAI
---


## What is Generative Modeling?

>[!note]
>_Generative modeling is a branch of [[ML|machine learning]]] that involves training a model to produce new data that is similar to a given dataset._


![[Pasted image 20231011202050.png]]
<small>Figure 1-1. A generative model trained to generate realistic photos of horses</small>


- Generative model must be _probabilistic_ rather than _deterministic_.
	- Generative model must include a random component that influences the individual samples generated by the model.

## Generative vs. Discriminative Modeling

![[Pasted image 20231011202647.png]]
<small>Figure 1-2. A discriminative model trained to predict if a given image is painted by Van Gogh</small>

- _Discriminative modeling estimates_ $p(y|x)$
- _Generative modeling estimates_ $p(x)$
	- Conditional generative models $p(x|y)$


## Core Probability Theory
- Generative modeling is closely connected to statistical modeling of probability distributions.

### Key terms:
#### _[[Sample Space]]_
- The sample space is the complete set of all values an observation $\mathbf{x}$ can take.

#### _[[Probability Density Function]]_:
- function $p(\mathbf{x})$ that maps a point $\mathbf{x}$ in the sample space to a number between 0 and 1.
- The integral of the density function over all points in the sample space must equal 1

- While there is only one true densify function $p_{data}(\mathbf{x})$ that is assumed to have generated the observable dataset
- There are infinitely many density functions $p_{\text{model}}(\mathbf{x})$ that we can use to estimate $p_{data}(\mathbf{x})$

#### _[[Parametric Modeling]]_:
- is a technique that we can use to structure our approach to finding a suitable $p_{\text{model}}(\mathbf{x})$.
- is a family of density functions $p_{\theta}(\mathbf{x})$ that can be described using a finite number of parameters, $\theta$.

#### _Likelihood_:
- $\mathcal{L}(\theta|\mathbf{x}) = p_{\theta}(\mathbf{x})$
- $\mathcal{L}(\theta|\mathbf{X}) = \prod_{\mathbf{x} \in \mathbf{X}}p_{\theta}(\mathbf{x})$
- Since the product of a large number of terms between 0 and 1 can be quite computationally difficult to work with, we often use _[[Log likelihood]]_  instead.
	- $\mathcal{l}(\theta|\mathbf{X}) = \sum_{\mathbf{x} \in \mathbf{X}}\log p_{\theta}(\mathbf{x})$

>[!warning]
>Note that the likelihood is a function of the _parameters_, not the data. It should not be interpreted as the probability that a given parameter set is correct - in other words, it is not the probability distribution over the parameter space (i.e., it doesn't sum/integrate to 1, with respect to the parameters).

The focus of parametric modeling should be to find the optimal value $\hat{\theta}$ of the parameter set that maximizes the likelihood of observing the dataset $\mathbf{X}$.

#### _Maximum likelihood estimation:_
- is the technique that allows us to estimate $\hat{\theta}$ - the set of parameters $\theta$ of a density function $p_{\theta}(\mathbf{x})$ that is most likely to explain some observed data $\mathbf{X}$.
$$
\begin{align}
\hat{\theta} = \underset{\mathbf{x}}{\operatorname{argmax}}\mathcal{l}(\theta\mid\mathbf{X})
\end{align}
$$
- $\hat{\theta}$ is also called the _maximum likelihood estimate_ ([[MLE]])

Neural networks typically _minimize_ a loss function, so we can equivalently talk about finding the set of parameters that _minimize_ the _negative log-likelihood_:
$$
\begin{align}
\hat{\theta} = \underset{\theta}{\operatorname{argmin}} - \mathscr{l}(\theta\mid \mathbf{X}) = \underset{\theta}{\operatorname{argmin}} - \log p_{\theta}(\mathbf{X})
\end{align}
$$
---
>[!note]
>Generative modeling can be thought of as a form of maximum likelihood estimation, where the parameters $\theta$ are the weights of the [[NN]] contained in the model.

For high-dimensional problems, it is generally not possible to directly calculate $p_{\theta}(\mathbf{x})$ - it is _intractable_.


## Generative Model Taxonomy
While all types of generative models ultimately aim to solve the same task, they all take slightly different approaches to modeling the density function $p_{\theta}(\mathbf{x})$. 

Broadly speaking, there are three possible approaches:
1. Explicitly model the density function, but constrain the model in some way, so that the density function is tractable (i.e., it can be calculated).
2. Explicitly model a tractable approximation of the density function.
3. Implicitly model the density function, through a stochastic process that directly generates data.

![[Pasted image 20231011215203.png]]

![[Pasted image 20231011215732.png]]


#### Explicit Density Model: 학습 Data 분포에 대한 Model을 확실히 정의할 수 있고, 이를 활용하여 생성하는 Generative Model
- [[VAE]]
	- VAE는 Data 분포를 학습할 수 있는 Data에 대하여 Data 분포를 학습하고, Data 분포가 잘 학습된다면 Sampling은 자동으로 수행되는 것이다.  
	- ([[Sampling]] : 내가 원하는 Data를 Generative Model을 활용하여 "생성"시키는 것)

#### Implicit Density Model: 학습 Data에 대한 Model을 정의하지 않고, 확률 분포를 파악하기 위해 Sample을 뽑는 방식
- Don't aim to estimate the probability density at all
- Focus solely on producing a stochastic process that directly generates data.

- [[GAN]]
	- Data 분포에는 큰 관심이 없으며, 단지 진짜 같은 Sample Data를 Generate하는 것을 목적으로 고안된 Sampler이다. 


